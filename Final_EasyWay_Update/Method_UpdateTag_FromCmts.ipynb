{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自動更新景點標籤 ( from 評論 ) :\n",
    "# 一天最多更新一次\n",
    "\n",
    "# 評論倒出資料庫 -> 評論撈出關鍵字 -> \n",
    "# 評論關鍵字分類 -> 評論貼完標籤 -> 輸出成JSON檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "\n",
    "#[1]\n",
    "####################################################################\n",
    "def DumpComment(IP,User,PWd,DB,sql,Folder,PrintReport):\n",
    "    import MySQLdb\n",
    "    import os\n",
    "    import re\n",
    "    db = MySQLdb.connect(IP,User,PWd,DB,charset='utf8')#連結資料庫\n",
    "    cursor = db.cursor()\n",
    "\n",
    "    count=0#　計算讀取的資料量\n",
    "    CntDic = {}#　Key:景點名稱　Value:評論數\n",
    "    Tmp = {}#　Key:景點編號　Value:景點名稱\n",
    "    os.popen('mkdir '+Folder)#　新增資料夾\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        for row in results: \n",
    "            if str(row[2]) in Tmp:#　如果已在Set裡           \n",
    "                CntDic[row[0].encode('utf-8')] += 1\n",
    "                with open(Folder+'\\\\'+str(row[2])+'.txt','a') as f:\n",
    "                    f.write(row[1].encode('utf-8')+'\\n')#　就繼續寫下去\n",
    "\n",
    "            else:#　如果還不在Tmp裡\n",
    "                Tmp[str(row[2])] = row[0].encode('utf-8')#　寫Tmp(Key:景點編號 Value:景點名稱)\n",
    "                CntDic[row[0].encode('utf-8')] = 1\n",
    "                with open(Folder+'\\\\'+str(row[2])+'.txt','w') as f2:\n",
    "                    f2.write('\\n'+row[0].encode('utf-8')+'\\n'\\\n",
    "                                    +row[1].encode('utf-8')+'\\n')#　把Title寫在第二行!\n",
    "            count += 1  \n",
    "\n",
    "        S1 = set() #存放全部'景點名稱'\n",
    "        S2 = set() #存放'重複的景點名稱'  \n",
    "        for a in Tmp:#　遍歷第一次Tmp 把重複的寫進S2的Set裡面\n",
    "            if Tmp[a] not in S1:#　如果是第一次讀到,則寫進S1裡面\n",
    "                S1.add(Tmp[a])\n",
    "            else:#　如果已經在S1裡(非首次讀到此\"景點名稱\",表出現重複景點名稱,加進S2裡)\n",
    "                S2.add(Tmp[a])\n",
    "\n",
    "        Duplicate = 0\n",
    "        for b in Tmp:#　遍歷第二次Tmp \n",
    "            if Tmp[b] in S2:#　如果已經在S2裡(表示出現重複的,則印出錯誤訊息!)\n",
    "                Duplicate += 1 #　代表出現重複\n",
    "                print b,Tmp[b],'  Duplicate!'#　印出重複訊息\n",
    "\n",
    "        if Duplicate >= 1:\n",
    "            #print 'Attraction Duplicate! Please Check!'\n",
    "            print '\\n\\nDuplicate!!!!!!!!!!!!!!!!\\n\\n'\n",
    "            ##### 刪掉連資源回收桶都找不到  慎用!!! #####\n",
    "            #也可以選擇 : 若出現重複...把全部txt檔都刪掉\n",
    "            #####command = 'cd {} & del *.txt'\n",
    "            #####os.popen(command.format(Folder))\n",
    "            ##########################################\n",
    "\n",
    "    except KeyError:\n",
    "        print \"Error: unable to fecth data\"\n",
    "\n",
    "    if PrintReport == 'Y':\n",
    "        print \"Num of Queries:\",str(count)\n",
    "        print 'Num of Attractions:',len(CntDic)\n",
    "        Cnt = 0\n",
    "        for i in CntDic:\n",
    "            Cnt += CntDic[i]\n",
    "            #####\n",
    "            #可決定是否要印出計算字典\n",
    "            #print i,str(CntDic[i]).decode('string_escape')\n",
    "            #####\n",
    "        print 'Num of Comments:',Cnt\n",
    "        if Cnt == count:\n",
    "            print 'Dump Comments Successfully!'\n",
    "####################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#[2]\n",
    "####################################################################\n",
    "def wordsToList(attfile):\n",
    "    import re\n",
    "    f = open(attfile,'r')\n",
    "    f1 = f.read()\n",
    "    f2 = ''.join(f1.split())\n",
    "    words=jieba.cut(f2)\n",
    "    \n",
    "    li = []\n",
    "    for u in words: #把\"有出現兩個連續英數字\"的詞語濾掉\n",
    "        if not re.search('(\\w)(\\w)',u):\n",
    "            li.append(u)\n",
    "    n1 = (' '.join(li)).encode('utf-8')\n",
    "    return n1\n",
    "    f.close()\n",
    "\n",
    "def stopWordsList(stopwordfile):\n",
    "    li = []\n",
    "    fid =  open(stopwordfile,'r')\n",
    "    for line in fid:\n",
    "        li.append(line.strip().decode('utf-8'))#要解開utf-8,才能成功給scikitlearn用\n",
    "    fid.close()\n",
    "    return li\n",
    "\n",
    "def SciNoEng(date,listInput,stopWordsFile,rankNum,title):\n",
    "    from sklearn import feature_extraction  \n",
    "    from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    filename = 'NE_SciTfidf-'+str(rankNum)+'-'+date+'.txt'\n",
    "    \n",
    "    #創造一個空矩陣\n",
    "    corpus = []\n",
    "    \n",
    "    #把傳入的List依序讀出，使用方法把這些東西全都讀出來\n",
    "    for each in range(0,len(listInput)):\n",
    "        corpus.append(wordsToList(listInput[each]))\n",
    "\n",
    "    vectorizer=CountVectorizer(analyzer='word',stop_words=stopWordsList(stopWordsFile))  #該類會將文本中的詞語轉換為詞頻矩陣，矩陣元素a[i][j] 表示j詞在i類文本下的詞頻  \n",
    "    x = vectorizer.fit_transform(corpus)  #將文本轉為詞頻矩陣 \n",
    "    transformer=TfidfTransformer()  #該類會統計每個詞語的tf-idf權值 \n",
    "    tfidf=transformer.fit_transform(x)  #計算tf-idf\n",
    "    word=vectorizer.get_feature_names()  #獲取詞袋模型中的所有詞語\n",
    "    weight=tfidf.toarray()   #將tf-idf矩陣抽取出來，元素a[i][j]表示j詞在i類文本中的tf-idf權重  \n",
    "    \n",
    "    with open(filename,'a') as files:\n",
    "        import operator\n",
    "        for i in range(len(weight)):\n",
    "            \n",
    "            #*#*#*#*#*#*#*#*#*#\n",
    "            #決定是否要印景點標題#\n",
    "            \n",
    "            #print listInput[i]\n",
    "            #files.write(listInput[i])\n",
    "            #files.write('\\n')\n",
    "            \n",
    "            TitleDic=['True','T','t','Yes','Y','y']\n",
    "            if title in TitleDic:\n",
    "                files.write(listInput[i])\n",
    "                #print listInput[i]\n",
    "                files.write('\\n')\n",
    "            else:\n",
    "                files.write('')\n",
    "            \n",
    "            #*#*#*#*#*#*#*#*#*#\n",
    "            dic = {}\n",
    "            for j in range(len(word)):  \n",
    "                dic[word[j]] = weight[i][j]\n",
    "            words_freq = sorted(dic.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
    "            num2 = 0\n",
    "            for ele in words_freq[0:rankNum]:\n",
    "                num2 += 1\n",
    "                if not ele[1] == float(0.0):\n",
    "                    #print num2,\"\\t\",ele[0],\"\\t\",ele[1]\n",
    "                    files.write(ele[0].encode('utf-8'))\n",
    "                    files.write(\"\\n\")\n",
    "                    \n",
    "def ExtractKeyWords(Folder,Rank,StopWords,PrintReport):\n",
    "    import time\n",
    "    import os\n",
    "    x = time.time()\n",
    "    lis = []\n",
    "    for name in os.listdir(Folder):\n",
    "        #path = Folder+'\\\\'+name\n",
    "        lis.append(Folder+'\\\\'+name)\n",
    "        \n",
    "    Date = time.strftime('%Y%m%d',time.localtime(time.time()))\n",
    "    \n",
    "    SciNoEng(Date,lis,StopWords,Rank,'Y')\n",
    "    y = time.time()\n",
    "    if PrintReport == 'Y':\n",
    "        print 'Extract Keywords Successfully!'\n",
    "        print 'Cost ',y-x,' secs.\\n'\n",
    "        \n",
    "####################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#[3]\n",
    "####################################################################\n",
    "def W2V_ClassificationMethod2(Concept,Corpus,SimLevel,SimPercent,Model):\n",
    "    \n",
    "    import gensim\n",
    "    #1__WIKI\n",
    "    if Model == 'Wiki':\n",
    "        model = gensim.models.Word2Vec.load(\"W2V_Wiki_Trastd/wiki.zh.sim.seg.trastd.model\")\n",
    "    #2__搜狐新闻数据\n",
    "    elif Model == 'SogouCS':\n",
    "        model = gensim.models.Word2Vec.load('W2V_SogouCS/SogouCS_Combine.model')\n",
    "    #3__全网新闻数据\n",
    "    elif Model == 'SogouCA':\n",
    "        model = gensim.models.Word2Vec.load('W2V_SogouCA/SogouCA_Combine.model')\n",
    "       \n",
    "    import operator\n",
    "    cnt = []#　　計算餵進來的總詞量\n",
    "    Expt = []#　　存W2V無法辨識的詞語\n",
    "    Remain = []#　　存沒有通過門檻的詞語\n",
    "    ClassingResult = {}\n",
    "\n",
    "    for z in Concept:#　　把概念標頭寫進字典\n",
    "        ClassingResult[z] = {}#　　每個字典的內容都是一個字典\n",
    "\n",
    "    for q in open(Corpus,'r'):#　　把萃取關鍵字的結果讀出來        \n",
    "        q = q.strip().decode('utf-8')#　　去空白,解碼\n",
    "        cnt.append(q)#　　把讀取內容寫進一陣列(計算樣本的資料量用)\n",
    "        try:\n",
    "            TmpDic = {}#　　暫時性的字典(比對詞語與各概念的相似度)\n",
    "            SameConcept = ''#　　用來存放概念,當q直接是某概念時使用\n",
    "            TmpWord = 'None'#　　用來比對詞語是不是與概念完全吻合,若是,None會被修改掉;反之則維持None(初值)\n",
    "\n",
    "            for a in Concept:#　　讀取各概念標頭\n",
    "                TmpList = []#　　暫時性的陣列(存放'詞語'vs'概念內各詞語'的相似度)     \n",
    "                for b in Concept[a]:#　　讀取個概念內的詞語\n",
    "\n",
    "                    #####完全ㄧ樣#####\n",
    "                    if q.encode('utf-8') == b:#　　如果等於\n",
    "                        #print '---目標詞「',q,'」就在【',a,'】的概念裡'\n",
    "                        TmpWord = b#　　把詞語指定給TmpWord\n",
    "                        break#　　跳出迴圈(已經知道跟某詞語完全一樣,就不用再比其他的詞語了)\n",
    "                    #####\n",
    "\n",
    "                    if model.similarity(q,b.decode('utf-8')) > SimLevel:#　　如果相似度達門檻\n",
    "                        TmpList.append(model.similarity(q,b.decode('utf-8')))#　　就把相似度寫進TmpList裡面\n",
    "\n",
    "                #####完全一樣#####        \n",
    "                if TmpWord != 'None':#　　不是None(初值)->被修改過->要完全相等才會出現此情況\n",
    "                    SameConcept = a#　　把現在比對到的概念( EX:輕鬆 )記起來\n",
    "                    break#　　跳出迴圈(已經知道是屬於哪個概念,就不用再比對其他概念了)\n",
    "                #####\n",
    "\n",
    "                if float(len(TmpList))/float(len(Concept[a])) > SimPercent:#　　如果達到相似成數的門檻\n",
    "                    #print '  -「',q,'」已過相似_百分比_門檻 :',SimPercent\n",
    "                    TmpAvg = 0.0#　　存放平均相似度\n",
    "                    for c in TmpList:#　　把每個相似度讀出來\n",
    "                        TmpAvg += c#　　加起來\n",
    "                    TmpDic[a] = float(TmpAvg)/float(len(TmpList))#　　把'總數'除以'陣列長度',得到平均相似度,\n",
    "                                                                    #再寫進字典裡,當q與多個概念都有相似度食\n",
    "                                                                    #可以比較與哪個概念的相似度比較高\n",
    "\n",
    "            #####完全一樣##### \n",
    "            if TmpWord != 'None': \n",
    "                ClassingResult[SameConcept][(q.encode('utf-8'))] = 1#　　把q寫進總字典的概念裡(Key:q Value:1)\n",
    "                continue\n",
    "            #####\n",
    "\n",
    "            if len(TmpDic) > 0:#　　如果暫存字典有任何一概念(表示q'至少'與一個概念相似)\n",
    "                #print '> >小結 :',q,'成功歸類！',\n",
    "                for d in TmpDic:#　　遍歷TmpDic\n",
    "                    if d == sorted(TmpDic.iteritems(),key=operator.itemgetter(1),reverse=True)[0][0]:\n",
    "                        #如果d = 排序後的第一名 (表示d就是與q最像的概念)\n",
    "                        if TmpDic[d] > SimLevel:#　　如果\"q與概念的相似度\"大於相似度門檻\n",
    "                            #print '類別為 :',d,'\\n'\n",
    "                            ClassingResult[d][q.encode('utf-8')] = TmpDic[d]#　　把q寫進總字典的概念裡(Key:q Value:相似度)\n",
    "                        else:#　　不太可能發生的情況\n",
    "                            Remain.append('WHAT!?'+q.encode('utf-8'))                    \n",
    "            else:#　　q與各概念相似度都沒有達標,無法歸類\n",
    "                Remain.append(q.encode('utf-8'))\n",
    "\n",
    "        except KeyError:#　　抓到KeyError( W2V不認識q )的例外處理\n",
    "            #print '「',q,'」EXCEPT! (KeyError)\\n'\n",
    "            Expt.append(q.encode('utf-8'))#　　把q加進例外的陣列理\n",
    "            \n",
    "#####　　把分類結果字典回傳　　#####\n",
    "    return ClassingResult\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "def AttrClassify(KeyWordsFile,Folder,PrintReport,PrintDict,ReturnDict,\\\n",
    "                     Concept,Corpus,SimLevel,SimPercent,Model):\n",
    "    \n",
    "    import re\n",
    "    from pprint import pprint\n",
    "    total = 0#　計算總共有多少行資料\n",
    "    title = 0#　計算有多少的景點\n",
    "    length = 0#　計算有多少個關鍵字\n",
    "\n",
    "    ReAttr = '('+Folder+')(.)(.*)'#　計算表頭，正規化用\n",
    "    for i in open(KeyWordsFile,'r'):#　打開結果的檔案 \n",
    "        if re.match(ReAttr,i.strip()):#　計算表頭(來源的資料夾)\n",
    "            title += 1#　表頭(景點)＋１\n",
    "        else:#　不是表頭。就是關鍵字\n",
    "            length += 1#　關鍵字＋１\n",
    "        total += 1#　全部＋１\n",
    "\n",
    "    if PrintReport == 'Y':\n",
    "        print \"景點總數:\",title,\" 關鍵字總數:\",length,\" 原始檔長度:\",total#　把數字都印出來\n",
    "        if (title+length) == total:#　如果 (景點+關鍵字) = (總行數)\n",
    "            print '1. 檔案讀取成功! 並無遺漏!\\n'#　代表沒有遺漏！成功讀取\n",
    "        else:\n",
    "            print '1. 錯誤! 請檢查!\\n'\n",
    "\n",
    "    domain = Folder+'\\\\'#  景點評論所在的資料夾(同時也是分割內容的標準)\n",
    "\n",
    "    with open(KeyWordsFile,'r') as fid:#　再讀一次結果的檔案\n",
    "        content = fid.read()#　把內容讀出來，存給content \n",
    "        DictA = {}#　創建一字典\n",
    "        c = 0#　另一變數c 初值為0\n",
    "\n",
    "        for i in content.split(domain):#　把內容依據CommentSample(存放景點評論的資料夾名稱)切割\n",
    "            if c > 0:#　因為split的結果，第0個(也就是for迴圈第一次進來)的值會是空的，所以至少要第二次進來的我才要\n",
    "                d = 0#　另一變數d 初值為0\n",
    "                Now = ''\n",
    "\n",
    "                for o in i.split():#　用空白把每個資料切開\n",
    "                    if d == 0:#　split的結果，第0個會是檔名(EX: 15.txt)，並不是關鍵字，要特別處理\n",
    "                        Now = o.strip()#　如果是表頭的話，就把文字指定給外面的變數Now\n",
    "                        DictA[Now] = []#　把表頭新增進字典裡，並指定內容為一陣列\n",
    "                    else:#　關鍵字都會進來這裡\n",
    "                        DictA[Now].append(o.strip())#　把關鍵字加進字典的Value陣列裡\n",
    "                    d+=1#　做完之後，把d加一。目的僅是因為首次進來(d=0)時，要為字典新增KEY\n",
    "                        #  以便之後的關鍵字有家可歸，之後繼續加上去並無實值意義\n",
    "            c+=1#　做完之後，把c加一。目的僅是為了避免第一次進來(c=0)的情況，之後繼續加上去並無實值意義\n",
    "\n",
    "    ######　　做完以上動作，會得到一字典DictA，KEY:(各景點編號.txt) Value:(各景點的關鍵字)\n",
    "\n",
    "    DictB = {}#  創建一字典(目的是要把DictA的KEY)轉換成中文\n",
    "    cnt = 0#  計算關鍵字數量用\n",
    "\n",
    "    for a in DictA:#  把字典的KEY(EX: 15.txt)讀出來\n",
    "        count = 0#  另一變數初值為 0\n",
    "        for u in open(domain+a,'r'):#  (資料夾+字典的KEY) = 檔案的相對位置；把檔讀出來 \n",
    "            count+=1#  每次進來都先+1，表是讀到第 N 行\n",
    "            if count == 2:#  當count = 2(讀到第二行, 也就是景點名稱)時，(假設讀到15.txt, 第二行是'陽明山')\n",
    "                DictB[u.strip()] = DictA[a]#  就把(15.txt)的Value指定給新的字典的'陽明山'這個KEY，達到檔名轉中文的目的\n",
    "                break#  得到中文景點名稱之後，就沒有必要讀下去，直接中斷此次迴圈\n",
    "        for b in DictA[a]:#  把每個KEY的關鍵字讀出來\n",
    "            cnt += 1#  計算關鍵字數量\n",
    "\n",
    "        #####印出原始字典(DictA)#####\n",
    "        if PrintDict == 'PrintA':\n",
    "            print a,'\\n',str(DictA[a]).decode('string_escape')\n",
    "        ###########################\n",
    "\n",
    "    if PrintReport == 'Y':\n",
    "        print 'DictA內含詞語總量: ',cnt\n",
    "        if cnt == length:\n",
    "            print '2. DictA寫入成功! 並無遺漏!\\n'\n",
    "        else:\n",
    "            print '2. 錯誤! DictA寫入過程出現遺漏!\\n'\n",
    "\n",
    "\n",
    "    cnt2 = 0#  計算關鍵字數量用\n",
    "    #####印出最終字典#####\n",
    "    for g in DictB:\n",
    "        if PrintDict == 'PrintB':\n",
    "            print g,'\\n',str(DictB[g]).decode('string_escape') \n",
    "    #####################\n",
    "        for r in DictB[g]:\n",
    "            cnt2 += 1\n",
    "\n",
    "    if PrintReport == 'Y':\n",
    "        print '\\nDictB內含詞語總量: ',cnt2\n",
    "        if cnt2 == cnt:\n",
    "            print '3. DictB寫入(轉換)成功! 並無遺漏!\\n'\n",
    "        else:\n",
    "            print '3. 錯誤! DictB寫入(轉換)過程出現遺漏!\\n'  \n",
    "            \n",
    "            \n",
    "###***************************************************\n",
    "    DictFinal = DictB\n",
    "    ClassingResult=W2V_ClassificationMethod2(Concept,Corpus,SimLevel,SimPercent,Model)    \n",
    "###***************************************************\n",
    "\n",
    "    if PrintDict == 'PrintConcept':\n",
    "        for ee in ClassingResult:\n",
    "            print ee,str(ClassingResult[ee]).decode('string_escape')\n",
    "    \n",
    "    \n",
    "    DictResult={}#　把景點關鍵字的結果轉成'概念'.'文字雲'\n",
    "    for a in DictFinal:#　把{景點:[關鍵字]}的字典讀出來\n",
    "        DictResult[a] = {'概　念':{},'文字雲':[]}#　指定KEY:景點 Value:概念{} 文字雲\n",
    "\n",
    "        for b in DictFinal[a]:#　把每個景點的關鍵字讀出來\n",
    "            z = 'Homeless'#　先假設每個關鍵字都沒有家\n",
    "\n",
    "            for c in ClassingResult:#　讀取每個概念,內容是{概念:{詞語:比重}}讀出來\n",
    "                if b in ClassingResult[c]:#　如果b(現在讀到的這個關鍵字)在某概念裡面\n",
    "                    z = 'Home'#　有家了\n",
    "\n",
    "                    for e in ClassingResult[c]:#　把概念的詞都讀一讀\n",
    "                        if b == e:#　如果'概念內的詞'='現在的關鍵字'-->抓到了!\n",
    "                            if not c in DictResult[a]['概　念']:#　如果景點還沒有此概念\n",
    "                                DictResult[a]['概　念'][c]={}#　為景點的此概念新增一個字典\n",
    "                                DictResult[a]['概　念'][c][b] = ClassingResult[c][e]# 並且把字詞寫進此景點的概念裡,KEY:字詞 Value:字詞與概念的相似度    \n",
    "                            else:#　如果景點已經有此概念了\n",
    "                                DictResult[a]['概　念'][c][b] = ClassingResult[c][e]#　把字詞寫進此景點的概念裡,KEY:字詞 Value:字詞與概念的相似度\n",
    "                            break#　因為以經分好家了,寫完之後就跳出迴圈\n",
    "                    break#　因為已分好家,所以再次跳出迴圈\n",
    "\n",
    "            if z == 'Homeless':#　如果字詞並沒有屬於任何概念,依然沒有家\n",
    "                DictResult[a]['文字雲'].append(b)#　就把字詞寫進該景點的'文字雲'裡面\n",
    "                \n",
    "                \n",
    "    #驗證結果\n",
    "    if PrintReport == 'Y':\n",
    "        Class = 0\n",
    "        Cloud = 0\n",
    "        for g in DictResult:\n",
    "            for h in DictResult[g]:\n",
    "                if h == '文字雲':\n",
    "                    Cloud += len(DictResult[g][h])\n",
    "                if h == '概　念':\n",
    "                    for k in DictResult[g][h]:\n",
    "                        Class += len(DictResult[g][h][k])\n",
    "                        \n",
    "        print 'DictReSult內含詞語總量: ',(Class + Cloud)\n",
    "        if (Class + Cloud) == cnt2:\n",
    "            print '4. DictReSult寫入(轉換)成功! 並無遺漏!\\n'\n",
    "        else:\n",
    "            print '4. 錯誤! DictReSult寫入(轉換)過程出現遺漏!\\n'\n",
    "\n",
    "\n",
    "    if PrintDict == 'PrintC':\n",
    "        for g in DictResult:\n",
    "            print '【',g,'】',\n",
    "            for u in DictResult[g]['概　念']:\n",
    "                print u,\n",
    "            print '\\n'        \n",
    "            \n",
    "    if PrintDict == 'PrintD':\n",
    "        for g in DictResult:\n",
    "            print '【',g,'】'\n",
    "            for h in DictResult[g]:\n",
    "                print '｢',h,'｣'\n",
    "\n",
    "                if h == '概　念':\n",
    "                    for k in DictResult[g][h]:\n",
    "                        print k,len(DictResult[g][h][k]),str(DictResult[g][h][k]).decode('string_escape')\n",
    "                elif h == '文字雲':\n",
    "                    print str(DictResult[g][h]).decode('string_escape')\n",
    "            print ''\n",
    "\n",
    "    ###決定是否回傳字典\n",
    "    \n",
    "    if ReturnDict == 'ReturnA':\n",
    "        return DictA\n",
    "    elif ReturnDict == 'ReturnB':\n",
    "        return DictB\n",
    "    elif ReturnDict == 'ReturnC':\n",
    "        return DictResult\n",
    "    elif ReturnDict == 'ReturnConcept':\n",
    "        return ClassingResult\n",
    "    \n",
    "####################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#[4]\n",
    "####################################################################    \n",
    "    \n",
    "def DumpWC(Result,WordCloudFile):\n",
    "    import json\n",
    "    x = 0\n",
    "    y = 0\n",
    "    WordCloud = {}\n",
    "    for a in Result:#　讀取結果的各個景點\n",
    "        for b in Result[a]:#　讀取各個景點的Value\n",
    "            if b == '文字雲':\n",
    "                for c in Result[a][b]:#　讀取文字雲的內容(陣列)\n",
    "                    if not c in WordCloud:#　如果是第一次讀到\n",
    "                        WordCloud[c] = 1#　則加進字典, 計算次數為一\n",
    "                    else:#　如果已經讀過了\n",
    "                        WordCloud[c] += 1#　則字典該詞語次數加一\n",
    "                x += 1#　計算景點數\n",
    "                y += len(Result[a][b])#　計算文字雲的字彙量\n",
    "\n",
    "    z = 0\n",
    "    for e in WordCloud:\n",
    "        z += WordCloud[e]#　計算文字雲的字彙量\n",
    "\n",
    "    if z==y:\n",
    "        j_str = json.dumps(WordCloud)#　丟進一JSON格式的變數\n",
    "        with open(WordCloudFile,'w') as f:#　寫進Json檔裡\n",
    "            f.write(j_str)\n",
    "        print 'Attr :',x,'Words :',len(WordCloud),'TotalWords :',z\n",
    "        print 'Trans To WordCloud Dict Successfully!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概念字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Dic = {\n",
    "'藝文':['藝術','美術','戲劇','音樂'],\\\n",
    "    \n",
    "'信仰':['信仰','祭祀','祭拜','膜拜','信眾','信奉'],\\\n",
    "    \n",
    "'美食':['美食','爽口','美味','好吃'],\\\n",
    "'新奇':['新奇','新穎','獵奇','獨特','驚喜'],\\\n",
    "\n",
    "'輕鬆':['輕鬆','休閒','遊憩','悠閒','休憩','消遣','愜意','放鬆'],\\\n",
    "'遊玩':['遊玩','好玩','遊樂'],\\\n",
    "'快樂':['快樂','愉快','開心','歡樂','幸福','美好'],\\\n",
    "'有趣':['有趣','趣味'],\\\n",
    "\n",
    "'脫俗':['脫俗','幽雅','清幽','優雅','氣質','心曠神怡'],\\\n",
    "'樂活':['環保','綠化','清新'],\\\n",
    "\n",
    "'安靜':['安靜','寧靜'],\\\n",
    "'乾淨':['乾淨','整潔','潔淨'],\\\n",
    "\n",
    "'懷舊':['懷舊','復古','情調'],\\\n",
    "'古老':['古老','悠久'],\\\n",
    "\n",
    "'髒亂':['骯髒','很髒','髒亂','低劣','灰塵','破舊'],\\\n",
    "\n",
    "'購物':['商圈','百貨','商場'],\\\n",
    "'特產':['特產','紀念品'],\\\n",
    "\n",
    "'運動':['運動','健康'],\\\n",
    "\n",
    "'漂亮':['漂亮','可愛','美麗','優美'],\\\n",
    "'文青':['清新','優雅','氣質'],\\\n",
    "\n",
    "'壯觀':['雄偉','壯觀','壯麗']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 設定參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\BigData\\dict.big.tra.txt ...\n",
      "DEBUG:jieba:Building prefix dict from C:\\Users\\BigData\\dict.big.tra.txt ...\n",
      "Loading model from cache c:\\users\\bigdata\\appdata\\local\\temp\\jieba.ud8ec30fabaf60e161f06b3552aab2f0e.cache\n",
      "DEBUG:jieba:Loading model from cache c:\\users\\bigdata\\appdata\\local\\temp\\jieba.ud8ec30fabaf60e161f06b3552aab2f0e.cache\n",
      "Loading model cost 0.270 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.270 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# Step 1 　從資料庫把東西倒出來\n",
    "# [一] #\n",
    "###　　　資料庫連結資訊　　　###\n",
    "IP   = \"10.120.26.4\"\n",
    "User = \"iii\"\n",
    "PWd  = \"iii\"\n",
    "DB   = \"iii\"\n",
    "###　　　###　　　###　　　####\n",
    "# [二] #\n",
    "######　　　SQL命令　　　######\n",
    "sql = \"SELECT a.Attr_name, c.C_content, a.Attr_id\\\n",
    "       FROM  `iiimap_attraction` AS a\\\n",
    "       JOIN  `iiimap_comment` AS c ON a.Attr_id = c.Attr_id\\\n",
    "       ORDER BY a.Attr_name ASC;\"\n",
    "###　　　###　　　###　　　####\n",
    "#############################################################\n",
    "\n",
    "\n",
    "# Step 2 　淬取關鍵字\n",
    "# [一] #\n",
    "######　　決定淬取排名　　######\n",
    "Rank = 30\n",
    "StopWords = 'Stop_Edit_Final_20160114.txt'\n",
    "# [二] #\n",
    "######　　　讀取字典　　　######\n",
    "import os\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import jieba.posseg as pseg  \n",
    "jieba.set_dictionary('dict.big.tra.txt')  #預設字典\n",
    "jieba.load_userdict('dict_twstd_tfidf.txt')  #中文分詞詞庫(TFIDF)\n",
    "jieba.load_userdict('dict_ntusd_pos.txt') #NTUSD_負向\n",
    "jieba.load_userdict('dict_ntusd_nag.txt') #NTUSD_正向\n",
    "jieba.load_userdict('dict_twedu_dict.txt') #教育部《重編國語辭典 修訂本》\n",
    "jieba.load_userdict('dict_Custom_20160114.txt') # 自定義字典\n",
    "jieba.analyse.set_stop_words(\"Stop_Edit_Final_20160114.txt\") #停用詞\n",
    "#############################################################\n",
    "\n",
    "\n",
    "# Step 3 　更新景點標籤\n",
    "import time\n",
    "Date = time.strftime('%Y%m%d',time.localtime(time.time()))\n",
    "# [一] #\n",
    "######　　　輸出相關參數　　　######\n",
    "KeyWordsFile = 'NE_SciTfidf-'+str(Rank)+'-'+Date+'.txt'\n",
    "PrintDict = 'N'\n",
    "ReturnDict = 'ReturnC'\n",
    "# [二] #\n",
    "######　　　W2V相關參數　　　######\n",
    "Concept   = Dic\n",
    "Corpus    = 'NE_SciTfidf-'+str(Rank)+'-'+Date+'.txt'\n",
    "SimLevel  = 0.35\n",
    "SimPercent= 0.49\n",
    "Model     = 'Wiki'\n",
    "#############################################################\n",
    "\n",
    "\n",
    "# Step 4 　決定重要資訊\n",
    "#　　要放置評論的資料夾\n",
    "Folder = 'Comment_'+Date\n",
    "#　　印出報告\n",
    "#Step 1 - 2\n",
    "PrintReport = 'Y'\n",
    "#Step 3 - 4\n",
    "PrintReport2 = 'N'\n",
    "#　　JSON檔案命名\n",
    "JsonFile = 'CommentTags-'+Date+'.json'#　　　　標籤結果\n",
    "WordCloudFile = 'WordCloud-'+Date+'.json'#　　 文字雲檔案\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用方法 --> 一次更新\n",
    "需要檔案：\n",
    "\n",
    "    一、字典、停用詞\n",
    "    二、Word2Vec模型\n",
    "    \n",
    "使用結果：\n",
    "\n",
    "    產生下列資料：\n",
    "       1.'Comment_...'[資料夾]：景點的所有評論\n",
    "       2.'NE_SciTfidf-'[TXT檔]：景點的所有關鍵字\n",
    "       3.'CommentTags-'[json檔]：景點的標籤結果\n",
    "       4.'WordCloud-..'[json檔]：文字雲數據(KEY:文字 VALUE:字頻)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Update Tags...\n",
      "\n",
      "Step1 Dump Comments...\n",
      "Num of Queries: 52660\n",
      "Num of Attractions: 226\n",
      "Num of Comments: 52660\n",
      "Dump Comments Successfully!\n",
      "Step1 Complete!\n",
      "\n",
      "Strp2 Extract Keywords...\n",
      "Extract Keywords Successfully!\n",
      "Cost  23.9499998093  secs.\n",
      "\n",
      "Step2 Complete!\n",
      "\n",
      "Strp3 Update Attraction Tags...\n",
      "Step3 Complete!\n",
      "\n",
      "Strp4 Dump Taggin Result Into Json Files...\n",
      "Step4 Complete!\n",
      "\n",
      "Strp5 Dump WordCloud Result Into Json Files...\n",
      "Attr : 226 Words : 2905 TotalWords : 3715\n",
      "Trans To WordCloud Dict Successfully!\n",
      "Step5 Complete!\n",
      "\n",
      "All Complete! Cost 125.044000149 secs.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a = time.time()\n",
    "print 'Start Update Tags...\\n'\n",
    "\n",
    "print 'Step1 Dump Comments...'\n",
    "DumpComment(IP,User,PWd,DB,sql,Folder,PrintReport)\n",
    "print 'Step1 Complete!\\n'\n",
    "\n",
    "print 'Strp2 Extract Keywords...'\n",
    "ExtractKeyWords(Folder,Rank,StopWords,PrintReport)\n",
    "print 'Step2 Complete!\\n'\n",
    "\n",
    "print 'Strp3 Update Attraction Tags...'\n",
    "Result = AttrClassify(KeyWordsFile,Folder,PrintReport2,PrintDict,ReturnDict,Concept,Corpus,SimLevel,SimPercent,Model)\n",
    "print 'Step3 Complete!\\n'\n",
    "\n",
    "print 'Strp4 Dump Taggin Result Into Json Files...'\n",
    "import json\n",
    "json_str = json.dumps(Result)#　丟進一JSON格式的變數\n",
    "with open(JsonFile,'w') as jfid:#　寫進Json檔裡\n",
    "    jfid.write(json_str)\n",
    "print 'Step4 Complete!\\n'\n",
    "\n",
    "print 'Strp5 Dump WordCloud Result Into Json Files...'\n",
    "DumpWC(Result,WordCloudFile)\n",
    "print 'Step5 Complete!\\n'\n",
    "\n",
    "b = time.time()\n",
    "print 'All Complete! Cost',str(b-a),'secs.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 也可以把字典輸出成PICKLE檔，以供別的方法使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Export\n",
    "import pickle\n",
    "PickleFile = 'Pickle_0121'\n",
    "with open(PickleFile+'.p',\"wb\") as f:\n",
    "    pickle.dump(Result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "import pickle\n",
    "with open(PickleFile+'.p', 'rb') as f2:\n",
    "    data = pickle.load(f2)\n",
    "print type(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
